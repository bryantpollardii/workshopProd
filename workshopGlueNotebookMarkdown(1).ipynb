{
	"metadata": {
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		},
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# workshopGlueNotebookMarkDown\n================\n\n## Metadata\n--------\n\n* **Date Created:** 2024-03-19\n* **Author:** Bryant Pollard, II\n* **Environment:** AWS Glue using PySpark\n* **Purpose:** Glue Script to Extract Stock Financial Data From DynamoDB, Transform the Data, and Load into S3 public (read only) buckets\n* **Dependencies:**\n  * Required Libraries:\n    + PySpark\n    + AWSGlue\n    + Boto3\n    + BotoCore\n    + PyArrow\n  * AWS Services:\n    + DynamoDB\n    + S3\n    + IAM Roles with appropriate permissions\n\n## Table of Contents\n---------------\n\n1. [Setup and Initialization](#setup-and-initialization)\n2. [Data Extraction](#data-extraction)\n3. [Data Transformation](#data-transformation)\n4. [Data Loading](#data-loading)\n",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "markdown",
			"source": "## Setup and Initialization\n-------------------------\n\n### Environment Configuration\n\nThis notebook uses AWS Glue Studio with PySpark 3.0 for processing financial data. The environment is automatically configured with the necessary dependencies and permissions.\n\n### Required Permissions\n\nThe notebook requires:\n- Read access to DynamoDB tables containing financial data\n- Write access to target S3 buckets\n",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "# Configuration settings\n%idle_timeout 180\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\n# Standard library imports\nfrom io import StringIO, BytesIO\nfrom decimal import Decimal\n\n# Third-party library imports\nimport boto3\nfrom botocore.exceptions import ClientError\nfrom boto3.dynamodb.conditions import Key\n\n# PySpark imports\nfrom pyspark.sql.functions import col, coalesce, nanvl, lit\nfrom pyspark.sql.types import IntegerType\nimport pyspark.sql.functions as F\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import (\n    col, length, coalesce, regexp_replace, when, udf\n)\nfrom pyspark.sql.types import FloatType, StringType\n\n# AWSGlue imports\nfrom awsglue.context import GlueContext\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.job import Job\nfrom awsglue.utils import getResolvedOptions",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 5c6d933d-712e-40a2-856f-9ced0da58b64.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 180 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 5c6d933d-712e-40a2-856f-9ced0da58b64.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 4.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 5c6d933d-712e-40a2-856f-9ced0da58b64.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: None\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 5c6d933d-712e-40a2-856f-9ced0da58b64.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: None\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#This is what runs the session.\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Data Extraction\n--------------\n\n### Source System Details\n\n* DynamoDB Table Name: TABLE5\n* Key: PK and SK using a Single Table Design\n* Attributes:\n  + PK (string: Stock ticker symbol\n  + SK (number): Year\n  + many others...",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "# Creates a dynamic frame for ETL operation on the data. \n# The data source is a DynamoDB table. Then specify the table name, read throughput percentage, and set the split count.\n\n\n\nsource_dynamic_frame = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"dynamodb\",\n    connection_options={\n        \"dynamodb.input.tableName\": \"TABLE5\",\n        \"dynamodb.throughput.read.percent\": \"1.0\",\n        \"dynamodb.splits\": \"15\"\n    }\n)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Data Transformation\n-------------------\n\n### Transformation Rules\n\n1. Data Standardization\n   - Rename PK to stock_symbol and SK to year\n   - Preprends 20 to ensure that 2 digit years become 4 digit years in the year column\n   - Remove any duplicate rows\n   - Removes % from YEARLY PRICE VAR columns \n   - Combine all YEARLY PRICE VAR columns into price_var\n   - Convert the numbers with paranthesis () to negative values in the column Book Value per Share Growth",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "\n#Convert the DynamicFrame to a Spark DataFrame\ndf = source_dynamic_frame.toDF()\n\n# # Rename columns\ndf = df.withColumnRenamed(\"PK\", \"stock_symbol\")\ndf = df.withColumnRenamed(\"SK\", \"year\")\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Since there is no 1900's data we can safley make this transformation.\n\n#It essentially contcatinates \"20\" infront of the stirng in year, but only if the length of the string is 2.\n# Otherwise leave it as is.\n\ndf = df.withColumn(\n    \"year\",\n    F.when(F.length(F.col(\"year\")) == 2, F.concat(F.lit(\"20\"), F.col(\"year\")))\n     .otherwise(F.col(\"year\"))\n)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Remove all duplicate rows\ndf.dropDuplicates()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "DataFrame[Total current assets: string, Earnings Before Tax Margin: string, Weighted Average Shs Out (Dil): string, 2017 PRICE VAR [%]: string, Gross Profit: string, Sector: string, Net Income: string, Total assets: string, Other Assets: string, Net Debt: string, Financing Cash Flow: string, stock_symbol: string, 2019 PRICE VAR [%]: string, Net cash flow / Change in cash: string, Total debt: string, Book Value per Share Growth: string, Profit Margin: string, Interest Expense: string, Consolidated Income: string, Total non-current liabilities: string, EBITDA: string, Total current liabilities: string, Operating Cash Flow: string, Tax Liabilities: string, Long-term investments: string, Net Profit Margin: string, Revenue: string, Weighted Average Shs Out: string, EBIT Margin: string, Total shareholders equity: string, Investment purchases and sales: string, Stock-based compensation: string, Net Income Com: string, EPS: string, Total liabilities: string, Free Cash Flow: string, EBITDA Margin: string, Investments: string, EBIT: string, Income Tax Expense: string, 2015 PRICE VAR [%]: string, Gross Margin: string, Goodwill and Intangible Assets: string, SG&A Expense: string, Long-term debt: string, Investing Cash flow: string, Tax assets: string, Total non-current assets: string, Property, Plant & Equipment Net: string, Class: string, Receivables: string, 2018 PRICE VAR [%]: string, Issuance (repayment) of debt: string, Short-term debt: string, year: string, 2016 PRICE VAR [%]: string, Payables: string, Retained earnings (deficit): string, Cost of Revenue: string, EPS Diluted: string, Depreciation & Amortization: string, Other Liabilities: string, Cash and cash equivalents: string, Capital Expenditure: string, eBTperEBIT: string, Earnings Yield: string, Effect of forex changes on cash: string, Receivables Turnover: string, 3Y Revenue Growth (per Share): string, priceToOperatingCashFlowsRatio: string, Net Income Growth: string, nIperEBT: string, Weighted Average Shares Diluted Growth: string, Operating Cash Flow per Share: string, Tangible Asset Value: string, Capex to Operating Cash Flow: string, Days of Inventory on Hand: string, interestCoverage: string, 10Y Revenue Growth (per Share): string, 5Y Revenue Growth (per Share): string, Issuance (buybacks) of shares: string, Dividend per Share: string, Inventory Growth: string, Free Cash Flow Yield: string, Inventory Turnover: string, capitalExpenditureCoverageRatios: string, ROE: string, 5Y Shareholders Equity Growth (per Share): string, Price to Sales Ratio: string, ebitperRevenue: string, Net Income per Share: string, Net Debt to EBITDA: string, Other comprehensive income: string, daysOfSalesOutstanding: string, longtermDebtToCapitalization: string, Intangibles to Total Assets: string, 10Y Operating CF Growth (per Share): string, EBIT Growth: string, enterpriseValueMultiple: string, Debt to Assets: string, Dividend payments: string, payablesTurnover: string, PE ratio: string, Enterprise Value: string, cashRatio: string, assetTurnover: string, Deferred revenue: string, EV to Free cash flow: string, Weighted Average Shares Growth: string, Market Cap: string, R&D Expenses: string, Book Value per Share: string, Preferred Dividends: string, PTB ratio: string, cashFlowToDebtRatio: string, ebtperEBIT: string, Average Inventory: string, Free Cash Flow growth: string, Graham Net-Net: string, dividendYield: string, priceToSalesRatio: string, priceToBookRatio: string, PFCF ratio: string, EPS Diluted Growth: string, Shareholders Equity per Share: string, Acquisitions and disposals: string, priceSalesRatio: string, 3Y Operating CF Growth (per Share): string, Days Payables Outstanding: string, Cash and short-term investments: string, Revenue Growth: string, 3Y Net Income Growth (per Share): string, Net Cash/Marketcap: string, Debt Growth: string, Operating Expenses: string, Free Cash Flow margin: string, priceEarningsToGrowthRatio: string, effectiveTaxRate: string, Operating Income Growth: string, EV to Sales: string, Tangible Book Value per Share: string, priceFairValue: string, Net Income - Non-Controlling int: string, Revenue per Share: string, priceEarningsRatio: string, Cash per Share: string, Operating Income: string, Enterprise Value over EBITDA: string, Free Cash Flow per Share: string, Deposit Liabilities: string, Capex to Depreciation: string, dividendpaidAndCapexCoverageRatios: string, Income Quality: string, operatingProfitMargin: string, niperEBT: string, fixedAssetTurnover: string, Average Payables: string, daysOfInventoryOutstanding: string, Capex per Share: string, debtEquityRatio: string, currentRatio: string, Stock-based compensation to Revenue: string, Inventories: string, grossProfitMargin: string, operatingCashFlowPerShare: string, 5Y Dividend per Share Growth (per Share): string, pretaxProfitMargin: string, PB ratio: string, debtRatio: string, EPS Growth: string, Days Sales Outstanding: string, Interest Debt per Share: string, Invested Capital: string, quickRatio: string, 10Y Dividend per Share Growth (per Share): string, Short-term investments: string, Payout Ratio: string, priceToFreeCashFlowsRatio: string, 10Y Net Income Growth (per Share): string, netProfitMargin: string, Receivables growth: string, Gross Profit Growth: string, priceBookValueRatio: string, R&D Expense Growth: string, cashFlowCoverageRatios: string, daysOfPayablesOutstanding: string, Net Current Asset Value: string, Earnings before Tax: string, SG&A Expenses Growth: string, 3Y Shareholders Equity Growth (per Share): string, Net Income - Discontinued ops: string, freeCashFlowOperatingCashFlowRatio: string, Operating Cash Flow growth: string, payoutRatio: string, SG&A to Revenue: string, 10Y Shareholders Equity Growth (per Share): string, Interest Coverage: string, POCF ratio: string, Graham Number: string, EV to Operating cash flow: string, shortTermCoverageRatios: string, Average Receivables: string, cashPerShare: string, inventoryTurnover: string, returnOnEquity: string, R&D to Revenue: string, Capex to Revenue: string, eBITperRevenue: string, operatingCashFlowSalesRatio: string, dividendPayoutRatio: string, companyEquityMultiplier: string, totalDebtToCapitalization: string, 5Y Operating CF Growth (per Share): string, 3Y Dividend per Share Growth (per Share): string, Payables Turnover: string, Dividend Yield: string, Dividends per Share Growth: string, 5Y Net Income Growth (per Share): string, Asset Growth: string, Debt to Equity: string, freeCashFlowPerShare: string, priceCashFlowRatio: string, Current ratio: string, ROIC: string, returnOnAssets: string, returnOnCapitalEmployed: string, Working Capital: string, Return on Tangible Assets: string, cashConversionCycle: string, operatingCycle: string]\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# List of columns to modify\ncolumns_to_modify = [\n    \"2015 PRICE VAR [%]\",\n    \"2016 PRICE VAR [%]\",\n    \"2017 PRICE VAR [%]\",\n    \"2018 PRICE VAR [%]\",\n    \"2019 PRICE VAR [%]\"\n]\n\n# Loop through the columns_to_modify and remove the % sign.\nfor column in columns_to_modify:\n    df = df.withColumn(column, regexp_replace(col(column), \"%\", \"\"))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#To avoid complexity in this demo, there is no need for null handling. \n\n# List of columns to manipulate\n\nprice_var_columns = [\n    \"2015 PRICE VAR [%]\",\n    \"2016 PRICE VAR [%]\",\n    \"2017 PRICE VAR [%]\",\n    \"2018 PRICE VAR [%]\",\n    \"2019 PRICE VAR [%]\"\n]\n\n# Convert columns to double (nullifying non-convertible values)\nfor column in price_var_columns:\n    df = df.withColumn(column, col(column).cast(\"double\"))\n\n# Use nanvl to replace NaN with null, and coalesce to get the first non-null value\ndf = df.withColumn(\n    \"price_var\", \n    coalesce(*[nanvl(col(c), lit(None)) for c in price_var_columns])\n)\n\n# Drop the original price variance columns\n\n# Show only the price_var column\ndf.select(\"price_var\").show()\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+\n|          price_var|\n+-------------------+\n| -4.950494862528325|\n| 23.927435714883966|\n| -32.84550178011025|\n|  -4.43052670613731|\n|-22.142853432772046|\n|  60.91953660745985|\n|  53.53938838025913|\n| -27.52227994058048|\n|  80.34036321109737|\n| -66.61308983517486|\n| -1.401875544864496|\n| 24.988463274349076|\n|  6.998880080021027|\n|  31.91400622758113|\n| -38.73831437694461|\n| 11.438928709783278|\n|  29.33753331091046|\n| -62.33296293845746|\n| 121.29669023955697|\n| -40.05208028738028|\n+-------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df = df.drop(*price_var_columns)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\n#Column to modify\ncolumn_name = \"Book Value per Share Growth\"\n\n# Remove parentheses and convert to negative float. This needs to be a float to multiply by -1.\n\ndf = df.withColumn(\n    column_name,\n    when(\n        col(column_name).rlike(r\"^\\(.*\\)$\"),  # Check if value is enclosed in parentheses\n        regexp_replace(col(column_name), r\"[()]\", \"\").cast(\"float\") * -1 \n    ).otherwise(\n        col(column_name).cast(\"float\")  # Convert to float for non-parenthesized values\n    )\n)\n\n\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#convert to a dynamic frame.\ndynamic_frame = DynamicFrame.fromDF(df, glueContext, \"dynamic_frame\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "## Data Loading\n-------------\n\n### Target System Details\n\n* S3 Bucket: s3://combined-stock-financial-datac/\n* Format: CSV\n* Partition Keys:\n  + out/year_<year>_data (string): Stock ticker symbol",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "def write_filtered_year(dynamic_frame, year):\n    # Convert 'year' to string type (if needed)\n    # Convert to data frame.\n    df = dynamic_frame.toDF().withColumn(\"year\", col(\"year\").cast(\"string\"))\n    dynamic_frame = DynamicFrame.fromDF(df, glueContext, \"dynamic_frame\")\n\n    # Filter data to only contain where the year is the specified year.\n    filtered_dynamic_frame = dynamic_frame.filter(lambda x: x['year'] == str(year))\n\n    # If there is no data found in the year do nothing.\n    if filtered_dynamic_frame.count() == 0:\n        print(f\"No data found for year {year}, skipping write.\")\n        return  \n\n    # Repartition the dataframe by converting to dataframe.This will ensure only one file is writen to S3. \n    df_filtered = filtered_dynamic_frame.toDF().repartition(1)\n    filtered_dynamic_frame = DynamicFrame.fromDF(df_filtered, glueContext, \"filtered_dynamic_frame\")\n    \n    #Generate a unique file name based on the year input\n    \n    output_file_name = f\"year_{year}_data\"\n    full_s3_path = f\"s3://combined-stock-financial-data/{output_file_name}\"\n    \n    # Write to S3\n    glueContext.write_dynamic_frame.from_options(\n        frame=filtered_dynamic_frame,\n        connection_type=\"s3\",\n        connection_options={\"path\": full_s3_path},\n        format=\"csv\",\n        format_options={\"header\": \"true\"}\n    )\nyears = (2014, 2015, 2016, 2017, 2018)\n# Process all years\nfor year in years:\n    write_filtered_year(dynamic_frame, year)\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def rename_single_s3_file(bucket_name, folder_prefix, target_key):\n    try:\n        # Initialize S3 client\n        s3_client = boto3.client('s3')\n        \n        # List all objects in the bucket, including the folder\n        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_prefix)\n        \n        # Check if there are any objects in the bucket\n        if 'Contents' not in response:\n            raise ValueError(\"No files found in the bucket\")\n        \n        # Check if there's exactly one object\n        if len(response['Contents']) != 1:\n            raise ValueError(\"Expected exactly one file, but found {} files\".format(len(response['Contents'])))\n        \n        # Get the name of the single file\n        source_key = response['Contents'][0]['Key']\n        \n        print(f\"Found single file: {source_key}\")\n        \n        # Rename the file\n        copy_source = {'Bucket': bucket_name, 'Key': source_key}\n        s3_client.copy_object(CopySource=copy_source, Bucket=bucket_name, Key=target_key)\n        \n        # Delete the original file\n        s3_client.delete_object(Bucket=bucket_name, Key=source_key)\n        \n        print(f\"Successfully renamed {source_key} to {target_key}\")\n    \n    except ClientError as e:\n        print(f\"An error occurred: {e}\")\n    except ValueError as ve:\n        print(ve)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Years to process\nyears = (2014, 2015, 2016, 2017, 2018)\n\n# Configuration for each year\nyear_configurations = {\n    #Details for each year \n    year: {\n        'bucket_name': 'combined-stock-financial-data',\n        'folder_prefix': f'year_{year}_data/',\n        'target_key': f'out/{year}_Financial_Data_DeDup.csv'\n    } for year in years\n}\ndef process_year(year):\n    # get the year configurations for the year. \n    config = year_configurations[year]\n    \n    \n    \n    # Rename single S3 file\n    rename_single_s3_file(config['bucket_name'], config['folder_prefix'], config['target_key'])\n\n# Process all years\nfor year in years:\n    process_year(year)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "Found single file: year_2014_data/run-1742419372769-part-r-00000\nSuccessfully renamed year_2014_data/run-1742419372769-part-r-00000 to out/2014_Financial_Data_DeDup.csv\nFound single file: year_2015_data/run-1742419395243-part-r-00000\nSuccessfully renamed year_2015_data/run-1742419395243-part-r-00000 to out/2015_Financial_Data_DeDup.csv\nFound single file: year_2016_data/run-1742419411811-part-r-00000\nSuccessfully renamed year_2016_data/run-1742419411811-part-r-00000 to out/2016_Financial_Data_DeDup.csv\nFound single file: year_2017_data/run-1742419426292-part-r-00000\nSuccessfully renamed year_2017_data/run-1742419426292-part-r-00000 to out/2017_Financial_Data_DeDup.csv\nFound single file: year_2018_data/run-1742419441446-part-r-00000\nSuccessfully renamed year_2018_data/run-1742419441446-part-r-00000 to out/2018_Financial_Data_DeDup.csv\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "job.commit()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}